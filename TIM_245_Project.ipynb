{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize(text, stopset):\n",
    "    tokens = casual_tokenize(text)\n",
    "    return [w for w in tokens if not w in stopset]\n",
    "\n",
    "df = pd.read_csv(\"lyrics.csv\")\n",
    "stopset = set(stopwords.words('english')).union(set(['chorus','verse','[',']','(',')']))\n",
    "df = df[['index','song','artist','genre','lyrics']]\n",
    "df = df[df.apply(lambda x: not (pd.isnull(x['lyrics']) or pd.isnull(x['song']) or x['genre'] == 'Not Available' or x['genre'] == 'Other'), \n",
    "        axis=1, reduce=True)]\n",
    "df['lyrics'] = df.apply(lambda x: tokenize(x['lyrics'], stopset), axis=1, reduce=True)\n",
    "df = df[df.apply(lambda x: len(x['lyrics']) > 20, axis=1, reduce=True)]\n",
    "df = df.groupby('genre').apply(lambda x: x.sample(n=2000))\n",
    "\n",
    "df.to_pickle('tickle_my.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word/Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.init_sims(replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create seeds\n",
    "happy = ['happy']\n",
    "love = ['love','lust']\n",
    "sad = ['sad']\n",
    "hate = ['anger','hate']\n",
    "\n",
    "#Generate related words from seeds\n",
    "happy_words50 = [a[0] for a in model.most_similar(positive=sad,negative=happy, topn=50)]\n",
    "sad_words50 = [a[0] for a in model.most_similar(positive=happy,negative=sad, topn=50)]\n",
    "love_words50 = [a[0] for a in model.most_similar(positive=love,negative=hate, topn=50)]\n",
    "hate_words50 = [a[0] for a in model.most_similar(positive=hate,negative=love, topn=50)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "\n",
    "def normalize(vector):\n",
    "    norm = linalg.norm(vector)\n",
    "    nrmlz = np.vectorize(lambda v: v/norm)\n",
    "    return nrmlz(vector)\n",
    "    \n",
    "def doc2vec(tokenized_doc, w2vmodel):\n",
    "    count = 0    \n",
    "    res = np.zeros(300)\n",
    "    for word in tokenized_doc:\n",
    "        if word not in string.punctuation:\n",
    "            if word in w2vmodel:\n",
    "                count += 1\n",
    "                res += w2vmodel[word]\n",
    "    if count != 0:\n",
    "        res /= count\n",
    "    return res\n",
    "\n",
    "happy_words50_vec = normalize(doc2vec(happy_words50,model))\n",
    "sad_words50_vec = normalize(doc2vec(sad_words50, model))\n",
    "love_words50_vec = normalize(doc2vec(love_words50, model))\n",
    "hate_words50_vec = normalize(doc2vec(hate_words50, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_pickle('tickle_my.pkl')['lyrics']\n",
    "df = df.apply(lambda x: normalize(doc2vec(x,model)))\n",
    "df = df.apply(lambda x: np.nan_to_num(x))\n",
    "df.to_pickle('purple.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from numpy import dot\n",
    "from numpy import linalg\n",
    "\n",
    "def cosine_simularity(a,b):\n",
    "    return dot(a, b)/(linalg.norm(a)*linalg.norm(b))\n",
    "\n",
    "df = pd.read_pickle('purple.pkl')\n",
    "\n",
    "happy = df.apply(lambda x: cosine_simularity(x,happy_words50_vec))\n",
    "happy.to_pickle('happy.pkl')\n",
    "\n",
    "sad = df.apply(lambda x: cosine_simularity(x,sad_words50_vec))\n",
    "sad.to_pickle('sad.pkl')\n",
    "\n",
    "love = df.apply(lambda x: cosine_simularity(x,love_words50_vec))\n",
    "love.to_pickle('love.pkl')\n",
    "\n",
    "hate = df.apply(lambda x: cosine_simularity(x,hate_words50_vec))\n",
    "hate.to_pickle('hate.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize(text, stopset):\n",
    "    tokens = casual_tokenize(text)\n",
    "    return [w for w in tokens if not w in stopset]\n",
    "\n",
    "stopset = set(stopwords.words('english')).union(set(['chorus','verse','[',']','(',')']))\n",
    "survey_lyrics = pd.read_csv('Lyricstest.csv')\n",
    "survey_lyrics['Lyrics'] = survey_lyrics.apply(lambda x: tokenize(x['Lyrics'], stopset), axis=1, reduce=True)\n",
    "survey_lyrics['Lyrics'] = survey_lyrics['Lyrics'].apply(lambda x: normalize(doc2vec(x,model)))\n",
    "\n",
    "happytest = survey_lyrics['Lyrics'].apply(lambda x: cosine_simularity(x,happy_words50_vec))\n",
    "happytest.to_pickle('happytest.pkl')\n",
    "\n",
    "sadtest = survey_lyrics['Lyrics'].apply(lambda x: cosine_simularity(x,sad_words50_vec))\n",
    "sadtest.to_pickle('sadtest.pkl')\n",
    "\n",
    "lovetest = survey_lyrics['Lyrics'].apply(lambda x: cosine_simularity(x,love_words50_vec))\n",
    "lovetest.to_pickle('lovetest.pkl')\n",
    "\n",
    "hatetest = survey_lyrics['Lyrics'].apply(lambda x: cosine_simularity(x,hate_words50_vec))\n",
    "hatetest.to_pickle('hatetest.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.226313\n",
       "1     0.240795\n",
       "2     0.247406\n",
       "3     0.350521\n",
       "4     0.284313\n",
       "5     0.200364\n",
       "6     0.253698\n",
       "7     0.235807\n",
       "8     0.222950\n",
       "9     0.241046\n",
       "10    0.280124\n",
       "11    0.224897\n",
       "12    0.267555\n",
       "13    0.244790\n",
       "14    0.305451\n",
       "Name: Lyrics, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('hatetest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Artist                       Song Title\n",
      "0                    Rolling Stones                     Satisfaction\n",
      "1                    Kelly Clarkson                   Because of You\n",
      "2                        Paula Cole  Where Have All the Cowboys Gone\n",
      "3           The Black Dahlia Murder                       Widowmaker\n",
      "4   Frank Carter & the Rattlesnakes                     Wild Flowers\n",
      "5                           Beatles             All You Need Is Love\n",
      "6           William Elliot Whitmore            There is Hope For You\n",
      "7                       David Bowie       The Man Who Sold the World\n",
      "8                        MC Lucious       Boom! I Got Your Boyfriend\n",
      "9                       Roy Orbison                   Running Scared\n",
      "10                      Patsy Cline                            Crazy\n",
      "11                         Tiny Tim           Living in the Sunlight\n",
      "12                        Metallica                    Enter Sandman\n",
      "13                      Ace of Base                         The Sign\n",
      "14                          50 Cent          Make Money By Any Means\n"
     ]
    }
   ],
   "source": [
    "print(survey_lyrics[['Artist','Song Title']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
